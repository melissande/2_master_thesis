{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s161362/.conda/envs/env_dhi/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "from numpy import newaxis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from dataset_generator_2 import DataLoader,Dataset_sat\n",
    "import h5py\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from imgaug import augmenters as iaa\n",
    "plt.ion()   # interactive mode\n",
    "from image_utils import read_data_h5,standardize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_INPUT='INPUT/'\n",
    "PATH_OUTPUT='OUTPUT/'\n",
    "NB_CLASSES=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flip(object):\n",
    "    \"\"\"Flip ratio of the image Left/Right and up/down\n",
    "\n",
    "    Args:\n",
    "        ratio (int): how much of the image is flipped\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,ratio):\n",
    "\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        X, Y = sample['input'], sample['groundtruth']\n",
    "        seq = iaa.Sequential([iaa.Fliplr(0.5),iaa.Flipud(0.5)])\n",
    "        data_tot=np.concatenate((X.astype('uint8'),Y.astype('uint8')),axis=2)\n",
    "        data_tot=seq.augment_images(data_tot[newaxis,:,:,:])\n",
    "        data_tot=np.squeeze(data_tot)\n",
    "        X=data_tot[:,:,:X.shape[2]]\n",
    "        Y=data_tot[:,:,X.shape[2]:]\n",
    "        return {'input': X, 'groundtruth': Y}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_image(path_input,path_output,nb_classes):\n",
    "    '''\n",
    "    Reads and saves as as an array image input and output\n",
    "    :paths_input path of the input image that have to be read  \n",
    "    :paths_output path of the output image that have to be read  \n",
    "    returns input and output image as array\n",
    "    '''\n",
    "    \n",
    "    X=read_data_h5(path_input)\n",
    "    Y_build=read_data_h5(path_output)\n",
    "    inp=torch.LongTensor(Y_build)\n",
    "    inp_ = torch.unsqueeze(inp, len(Y_build.shape))\n",
    "    Y = torch.FloatTensor(Y_build.shape[0],Y_build.shape[1],nb_classes ).zero_()\n",
    "    Y.scatter_(len(Y_build.shape), inp_, 1)\n",
    "    Y=np.asarray(Y)\n",
    "            \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_sat(Dataset):\n",
    "    \"\"\"Satellite images dataset with rastered footprints in groundtruth.\"\"\"\n",
    "\n",
    "    def __init__(self,paths_input: np.ndarray,paths_output: np.ndarray,nb_classes: int,transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            \n",
    "        \"\"\"\n",
    "        self.paths_input = paths_input\n",
    "        self.paths_output = paths_output\n",
    "        self.nb_classes=nb_classes\n",
    "        self.transform = transform\n",
    "    @classmethod\n",
    "    def from_root_folder(cls, root_folder: str, nb_classes: int,*,transform=None, max_data_size:  int = None):\n",
    "        paths_input = []\n",
    "        paths_output=[]\n",
    "        \n",
    "        \n",
    "        for filename in sorted(os.listdir(root_folder+PATH_INPUT))[:max_data_size]:\n",
    "            paths_input.append(os.path.join(root_folder+PATH_INPUT, filename))\n",
    "\n",
    "        for filename in sorted(os.listdir(root_folder+PATH_OUTPUT))[:max_data_size]:\n",
    "\n",
    "            paths_output.append(os.path.join(root_folder+PATH_OUTPUT, filename))\n",
    "        \n",
    "        \n",
    "        return Dataset_sat(np.asarray(paths_input), np.asarray(paths_output),nb_classes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths_input)\n",
    "    \n",
    "#     def shuffled(self):\n",
    "# #         if seed is not None:\n",
    "# #             np.random.seed(seed)\n",
    "\n",
    "#         idx = np.arange(len(self.paths_input))\n",
    "#         np.random.shuffle(idx)\n",
    "#         generator = Dataset_sat(self.paths_input[idx], self.paths_output[idx],self.nb_classes)\n",
    "\n",
    "#         return generator \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        X,Y=_parse_image(self.paths_input[idx],self.paths_output[idx],self.nb_classes)     \n",
    "        sample = {'input': X, 'groundtruth': Y}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 120, 120, 9]) torch.Size([4, 120, 120, 2])\n",
      "1 torch.Size([4, 120, 120, 9]) torch.Size([4, 120, 120, 2])\n",
      "2 torch.Size([4, 120, 120, 9]) torch.Size([4, 120, 120, 2])\n",
      "3 torch.Size([4, 120, 120, 9]) torch.Size([4, 120, 120, 2])\n"
     ]
    }
   ],
   "source": [
    "root_folder = '../2_DATA_GHANA/DATASET/120_x_120_8_pansh/TEST/'\n",
    "\n",
    "# composed = transforms.Compose([Rescale(256),transform=Flip(0.5)\n",
    "#                                RandomCrop(224)])\n",
    "generator=Dataset_sat.from_root_folder(root_folder,NB_CLASSES)\n",
    "    \n",
    "dataloader = DataLoader(generator, batch_size=4,shuffle=True, num_workers=4)\n",
    "\n",
    "# trsfrm=Flip(0.5)\n",
    "# sample_batch=generator.__getitem__(4)\n",
    "# plt.imshow(sample_batch['input'][:,:,0])\n",
    "# plt.show()\n",
    "# t_sample_batch = trsfrm(sample_batch)\n",
    "# plt.imshow(t_sample_batch['input'][:,:,0])\n",
    "# plt.show()\n",
    "    \n",
    "\n",
    "for i_batch,sample_batch in enumerate(dataloader):\n",
    "    print( i_batch,sample_batch['input'].size(),\n",
    "          sample_batch['groundtruth'].size())\n",
    "    \n",
    "    \n",
    "    if i_batch==3:\n",
    "        break\n",
    "        \n",
    "\n",
    "test2=standardize(sample_batch['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dhi",
   "language": "python",
   "name": "env_dhi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
