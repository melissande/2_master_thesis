{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import logging\n",
    "from image_utils import standardize\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as Fu\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from unet_val import UNet\n",
    "import torch.backends.cudnn as cudnn\n",
    "from dataset_generator_2 import DataLoader,Dataset_sat\n",
    "from IOU_computations import *\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "GLOBAL_PATH='MODEL_BASIC_TEST_120/'\n",
    "##########\n",
    "\n",
    "if not os.path.exists(GLOBAL_PATH):\n",
    "            os.makedirs(GLOBAL_PATH)\n",
    "        \n",
    "#############\n",
    "PATH_TRAINING='TRAINING/'\n",
    "PATH_VALIDATION='VALIDATION/'\n",
    "PATH_TEST='TEST/'\n",
    "\n",
    "PATH_INPUT='INPUT/'\n",
    "PATH_OUTPUT='OUTPUT/'\n",
    "##############\n",
    "\n",
    "        \n",
    "INPUT_CHANNELS=9\n",
    "OUTPUT_CHANNELS=2\n",
    "NB_CLASSES=2\n",
    "\n",
    "SIZE_PATCH=120\n",
    "##############\n",
    "MODEL_PATH_SAVE=GLOBAL_PATH+'RESUNET_pytorch_BASIC_test'\n",
    "MODEL_PATH_RESTORE=''\n",
    "TEST_SAVE=GLOBAL_PATH+'TEST_SAVE/'\n",
    "if not os.path.exists(TEST_SAVE):\n",
    "            os.makedirs(TEST_SAVE)\n",
    "        \n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "##############\n",
    "\n",
    "REC_SAVE=200#2000\n",
    "DROPOUT=0.9#0.9\n",
    "DEFAULT_BATCH_SIZE = 8#10\n",
    "DEFAULT_EPOCHS = 1#50\n",
    "DEFAULT_ITERATIONS =618#495\n",
    "DEFAULT_VALID=100#100\n",
    "DISPLAY_STEP=100#50\n",
    "\n",
    "###############\n",
    "DEFAULT_LAYERS=3\n",
    "DEFAULT_FEATURES_ROOT=32\n",
    "DEFAULT_FILTERS_SIZE=3\n",
    "DEFAULT_LR=0.0001\n",
    "\n",
    "####### TMP folder for IOU\n",
    "\n",
    "TMP_IOU=TEST_SAVE+'TMP_IOU/'\n",
    "if not os.path.exists(TMP_IOU):\n",
    "            os.makedirs(TMP_IOU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    " class Trainer(object):\n",
    "    \"\"\"\n",
    "    Trains a unet instance\n",
    "    \n",
    "    :param net: the unet instance to train\n",
    "    :param batch_size: size of training batch\n",
    "    :param lr: learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, net, batch_size=10, lr=0.0001,nb_classes=2):\n",
    "        self.net = net\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.nb_classes=nb_classes\n",
    "    def _initialize(self, prediction_path):\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.net.parameters(),lr=self.lr)\n",
    "        self.prediction_path = prediction_path\n",
    "        \n",
    "    \n",
    "    def train(self, data_provider_path, save_path='', restore_path='', training_iters=4, epochs=3, dropout=0.9, display_step=1, validation_batch_size=30,rec_save=1, prediction_path = '',data_aug=None):\n",
    "        \"\"\"\n",
    "        Lauches the training process\n",
    "        \n",
    "        :param data_provider_path: where the DATASET folder is\n",
    "        :param save_path: path where to store checkpoints\n",
    "        :param restore_path: path where is the model to restore is stored\n",
    "        :param training_iters: number of training mini batch iteration\n",
    "        :param epochs: number of epochs\n",
    "        :param dropout: dropout probability\n",
    "        :param display_step: number of steps till outputting stats\n",
    "        :param restore: Flag if previous model should be restored \n",
    "        :param prediction_path: path where to save predictions on each epoch\n",
    "        \"\"\"\n",
    "        \n",
    "        PATH_TRAINING=data_provider_path+'TRAINING/'\n",
    "        PATH_VALIDATION=data_provider_path+'VALIDATION/'\n",
    "        PATH_TEST=data_provider_path+'TEST/'\n",
    "        \n",
    "        loss_train,file_train,loss_verif,file_verif,IOU_verif,IOU_file_verif,IOU_acc_verif,IOU_acc_file_verif,f1_IOU_verif,f1_IOU_file_verif=save_metrics(epochs,training_iters,TEST_SAVE,'a')\n",
    "        \n",
    "        if epochs == 0:\n",
    "            return save_path\n",
    "        if save_path=='':\n",
    "            return 'Specify a path where to store the Model'\n",
    "        self._initialize(prediction_path)\n",
    "            \n",
    "        if restore_path=='':\n",
    "            print('Model trained from scratch')\n",
    "        else:            \n",
    "            self.net.load_state_dict(torch.load(restore_path))\n",
    "            print('Model loaded from {}'.format(restore_path))\n",
    "          \n",
    "        \n",
    "        val_generator=Dataset_sat.from_root_folder(PATH_VALIDATION,self.nb_classes)\n",
    "        val_loader = DataLoader(val_generator, batch_size=validation_batch_size,shuffle=False, num_workers=4)\n",
    "        RBD=randint(0,int(val_loader.__len__() /validation_batch_size))\n",
    "        self.store_init(val_loader,\"_init\",validation_batch_size,RBD)\n",
    "        \n",
    "        train_len = self.batch_size*training_iters\n",
    "        train_generator=Dataset_sat.from_root_folder(PATH_TRAINING,self.nb_classes)\n",
    "        \n",
    "\n",
    "        logging.info(\"Start optimization\")\n",
    "\n",
    "        counter=0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            train_loader = DataLoader(train_generator, batch_size=self.batch_size,shuffle=True, num_workers=4)\n",
    "            for i_batch,sample_batch in enumerate(train_loader):\n",
    "                batch_x=standardize(sample_batch['input'])\n",
    "                batch_y=sample_batch['groundtruth']\n",
    "                _,loss=predict(self.net,batch_x,batch_y)\n",
    "                total_loss+=loss.data[0]\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                counter+=1\n",
    "                if step % display_step == 0:\n",
    "                    self.output_minibatch_stats(i_batch,batch_x,batch_y)\n",
    "                if counter % rec_save == 0:\n",
    "                    torch.save(self.net.state_dict(),save_path + 'CP{}.pth'.format(counter))\n",
    "                    print('Checkpoint {} saved !'.format(counter))\n",
    "\n",
    "                \n",
    "            error_rate_v,loss_v,iou_v,iou_acc_v,f1_v=self.store_validation(val_loader, \"epoch_%s\"%epoch,validation_batch_size,RBD,True)\n",
    "            IOU_verif[epoch]=iou_v\n",
    "            IOU_acc_verif[epoch]=iou_acc_v\n",
    "            f1_IOU_verif[epoch]=f1_v\n",
    "            loss_verif[epoch]=loss_v\n",
    "            \n",
    "            IOU_file_verif.write(str(IOU_verif[epoch])+'\\n')\n",
    "            IOU_acc_file_verif.write(str(IOU_acc_verif[epoch])+'\\n')\n",
    "            f1_IOU_file_verif.write(str(f1_IOU_verif[epoch])+'\\n')\n",
    "            file_verif.write(str(loss_verif[epoch])+'\\n')\n",
    "            \n",
    "            print(\" Loss {:.4f}, Error rate {:.4f} ,Validation IoU {:.4f}, Validation IoU_acc {:.4f}%,Validation F1 IoU {:.4f}%\".format(loss_v,error_rate_v,iou_v,iou_acc_v,f1_v))\n",
    "    \n",
    "    def output_minibatch_stats(self, step, batch_x, batch_y):\n",
    "        # Calculate batch loss and accuracy\n",
    "        predictions,loss=self.predict(self.net,batch_x,batch_y)\n",
    "        loss=loss.data[0]\n",
    "        predictions=predictions.data.cpu().numpy()\n",
    "        groundtruth=np.asarray(batch_y)\n",
    "        logging.info(\"Iter {:}, Minibatch Loss= {:.4f}, Minibatch error= {:.1f}%\".format(step,loss,error_rate(predictions, groundtruth)))\n",
    "   \n",
    "    def store_train(self,train_loader,name,training_batch_size):\n",
    "        print('nada')\n",
    "        \n",
    "        \n",
    "    def store_init(self,val_loader,name,validation_batch_size,random_batch_display,*,save_patches=True):\n",
    "        loss_v=0\n",
    "        error_rate_v=0\n",
    "        for i_batch,sample_batch in enumerate(val_loader):\n",
    "            batch_x=standardize(sample_batch['input'])\n",
    "            probs,loss=predict(self.net,batch_x,sample_batch['groundtruth'])\n",
    "            loss_v+=loss.data[0]\n",
    "            prediction_v=probs.data.cpu().numpy()\n",
    "            groundtruth=np.asarray(sample_batch['groundtruth'])\n",
    "            error_rate_v+=error_rate(prediction_v,groundtruth)\n",
    "            if i_batch==random_batch_display and save_patches:\n",
    "                batch_x=np.asarray(batch_x)\n",
    "                pansharp=np.stack((batch_x[:,:,:,5],batch_x[:,:,:,3],batch_x[:,:,:,2]),axis=3)\n",
    "                plot_summary(prediction_v,groundtruth,pansharp,name,self.prediction_path,save_patches)\n",
    "        loss_v/=val_loader.__len__()   \n",
    "        error_rate_v/=val_loader.__len__()\n",
    "        logging.info(\"Verification  loss= {:.4f},error= {:.1f}%\".format(loss_v,error_rate_v))\n",
    "        \n",
    "    \n",
    "    def store_validation(self,val_loader, name,validation_batch_size,random_batch_display,*,save_patches=True):\n",
    "        loss_v=0\n",
    "        iou_v=0\n",
    "        iou_acc_v=0\n",
    "        f1_v=0\n",
    "        error_rate_v=0\n",
    "\n",
    "        for i_batch,sample_batch in enumerate(val_loader):\n",
    "            probs,loss=predict(self.net,sample_batch['input'],sample_batch['groundtruth'])\n",
    "            loss_v+=loss.data[0]\n",
    "            \n",
    "            prediction_v=probs.data.cpu().numpy()\n",
    "            groundtruth=np.asarray(sample_batch['groundtruth'])\n",
    "            iou_acc,f1,iou=predict_score_batch(TMP_IOU,np.argmax(groundtruth,3),np.argmax(prediction_v,3))\n",
    "            iou_acc_v+=iou_acc\n",
    "            iou_v+=iou\n",
    "            f1_v+=f1\n",
    "            error_rate_v+=error_rate(prediction_v,groundtruth)\n",
    "            if i_batch==random_batch_display and save_patches:\n",
    "                batch_x=np.asarray(sample_batch['input'])\n",
    "                pansharp=np.stack((batch_x[:,:,:,5],batch_x[:,:,:,3],batch_x[:,:,:,2]),axis=3)\n",
    "                plot_summary(prediction_v,groundtruth,pansharp,name,self.prediction_path,save_patches)\n",
    "\n",
    "        loss_v/=val_loader.__len__()  \n",
    "        iou_v/=val_loader.__len__()\n",
    "        iou_acc_v/=val_loader.__len__()\n",
    "        f1_v/=val_loader.__len__()\n",
    "        error_rate_v/=val_loader.__len__()\n",
    "\n",
    "        logging.info(\"Verification  loss= {:.4f},error= {:.1f}%, IOU = {:.4f}, IOU Precision = {:.4f}%, F1 IOU= {:.4f}%\".format(loss_v,error_rate_v,iou_v,iou_acc_v,f1_v))\n",
    "\n",
    "        return error_rate_v,loss_v,iou_v,iou_acc_v,f1_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net,batch_x,batch_y):\n",
    "    X=batch_x.permute(0,3,1,2)\n",
    "    X = Variable(X).type(torch.FloatTensor).cuda()\n",
    "    Y=batch_y.permute(0,3,1,2)\n",
    "    Y = Variable(Y).type(torch.FloatTensor).cuda()\n",
    "\n",
    "    y_pred=net(X)\n",
    "    probs = Fu.softmax(y_pred,dim=1)\n",
    "    loss=Fu.binary_cross_entropy_with_logits(probs,Y)\n",
    "    probs=probs.permute(0,2,3,1)\n",
    "    return probs,loss\n",
    "\n",
    "def save_metrics(epochs,training_iters,prediction_path,mode):\n",
    "    #STORE loss for ANALYSIS\n",
    "    loss_train=np.zeros(training_iters*epochs)\n",
    "    file_train = open(prediction_path+'loss_train.txt',mode) \n",
    "    loss_verif=np.zeros(epochs)\n",
    "    file_verif = open(prediction_path+'loss_verif.txt',mode) \n",
    "    #STORE IOU for ANALYSIS\n",
    "    IOU_verif=np.zeros(epochs)\n",
    "    IOU_file_verif = open(prediction_path+'iou_verif.txt',mode)\n",
    "    #STORE IOU_ACC for ANALYSIS\n",
    "    IOU_acc_verif=np.zeros(epochs)\n",
    "    IOU_acc_file_verif = open(prediction_path+'iou_acc_verif.txt',mode)\n",
    "    #STORE f1_IOU for ANALYSIS\n",
    "    f1_IOU_verif=np.zeros(epochs)\n",
    "    f1_IOU_file_verif = open(prediction_path+'f1_iou_verif.txt',mode) \n",
    "    \n",
    "    return loss_train,file_train,loss_verif,file_verif,IOU_verif,IOU_file_verif,IOU_acc_verif,IOU_acc_file_verif,f1_IOU_verif,f1_IOU_file_verif\n",
    "def error_rate(predictions, labels):\n",
    "    \"\"\"\n",
    "    Return the error rate based on dense predictions and 1-hot labels.\n",
    "    \"\"\"\n",
    "\n",
    "    return 100.0 - (\n",
    "        100.0 *\n",
    "        np.sum(np.argmax(predictions, 3) == np.argmax(labels, 3)) /\n",
    "        (predictions.shape[0]*predictions.shape[1]*predictions.shape[2]))\n",
    "def plot_summary(predictions,labels,pansharp,epoch,prediction_path,save_patches):\n",
    "    \n",
    "    fig,axs=plt.subplots(3, len(pansharp),figsize=(8*len(pansharp),24))\n",
    "\n",
    "    axs[0,0].set_title(epoch+' Pansharpened ', fontsize='large')\n",
    "    axs[1,0].set_title(epoch+' Groundtruth ', fontsize='large')\n",
    "    axs[2,0].set_title(epoch+' Predictions ', fontsize='large')\n",
    "\n",
    "    labels=np.argmax(labels, 3) \n",
    "    logits=np.argmax(predictions, 3)\n",
    "    for i in range(len(pansharp)):\n",
    "\n",
    "        axs[0,i].imshow(pansharp[i])\n",
    "        axs[1,i].imshow(labels[i]) \n",
    "        axs[2,i].imshow(logits[i])\n",
    "        \n",
    "        \n",
    "        if save_patches:\n",
    "            plt.imsave(prediction_path+epoch+'_Panchro_'+str(i)+'.jpg',pansharp[i])\n",
    "            plt.imsave(prediction_path+epoch+'_Groundtruth_'+str(i)+'.jpg',labels[i])\n",
    "            plt.imsave(prediction_path+epoch+'_Predictions_'+str(i)+'.jpg',1-logits[i])\n",
    "\n",
    "    plt.subplots_adjust()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained from scratch\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    model=UNet(INPUT_CHANNELS,NB_CLASSES,DEFAULT_LAYERS,DEFAULT_FEATURES_ROOT,DROPOUT)\n",
    "    model.cuda()\n",
    "    cudnn.benchmark = True\n",
    "#     root_folder ='/scratch/SPACENET_DATA_PROCESSED/DATASET/120_x_120_8_bands_pansh/'\n",
    "    root_folder = '../DATA_GHANA/DATASET/120_x_120_8_bands/'\n",
    "    trainer=Trainer(model,DEFAULT_BATCH_SIZE,DEFAULT_LR,NB_CLASSES)\n",
    "    trainer.train( root_folder, MODEL_PATH_SAVE, MODEL_PATH_RESTORE, DEFAULT_ITERATIONS,DEFAULT_EPOCHS,DROPOUT, DISPLAY_STEP, DEFAULT_VALID,REC_SAVE, TEST_SAVE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dhi",
   "language": "python",
   "name": "env_dhi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
