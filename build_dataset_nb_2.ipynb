{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "from image_utils import read_data_h5,write_data_h5\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading groundtruth.h5\n",
      "Reading ms_hr_0.h5\n",
      "Reading ms_hr_1.h5\n",
      "Reading ms_hr_2.h5\n",
      "Reading ms_hr_3.h5\n",
      "Reading ms_hr_4.h5\n",
      "Reading ms_hr_5.h5\n",
      "Reading ms_hr_6.h5\n",
      "Reading ms_hr_7.h5\n",
      "Reading panchro.h5\n",
      "Reading pansharpened_0.h5\n",
      "Reading pansharpened_1.h5\n",
      "Reading pansharpened_2.h5\n",
      "Reading pansharpened_3.h5\n",
      "Reading pansharpened_4.h5\n",
      "Reading pansharpened_5.h5\n",
      "Reading pansharpened_6.h5\n",
      "Reading pansharpened_7.h5\n",
      "(7747, 120, 120, 17)\n",
      "(7747, 120, 120)\n",
      "Dataset read\n",
      "Dataset shuffled\n",
      "Save indices of shuffle\n",
      "Split (TRAINING - VALIDATION:0.800000) - TEST:0.800000  done\n",
      "Training size:4958, Validation size:1240, Test size: 1549\n",
      "../2_DATA_GHANA/DATASET/120_x_120_8_pansh/\n",
      "../2_DATA_GHANA/DATASET/120_x_120_4_pansh/\n",
      "../2_DATA_GHANA/DATASET/120_x_120_8_ms/\n",
      "../2_DATA_GHANA/DATASET/120_x_120_8_pansh_8_ms/\n",
      "BUILD TRAINING SET\n",
      "Patch 0\n",
      "../2_DATA_GHANA/DATASET/120_x_120_8_pansh/TRAINING/INPUT/input_0.h5\n",
      "(120, 120, 9)\n",
      "../2_DATA_GHANA/DATASET/120_x_120_4_pansh/TRAINING/INPUT/input_0.h5\n",
      "(120, 120, 5)\n",
      "../2_DATA_GHANA/DATASET/120_x_120_8_ms/TRAINING/INPUT/input_0.h5\n",
      "(120, 120, 9)\n",
      "../2_DATA_GHANA/DATASET/120_x_120_8_pansh_8_ms/TRAINING/INPUT/input_0.h5\n",
      "(120, 120, 17)\n",
      "Patch 1\n",
      "../2_DATA_GHANA/DATASET/120_x_120_8_pansh/TRAINING/INPUT/input_1.h5\n",
      "(120, 120, 9)\n",
      "../2_DATA_GHANA/DATASET/120_x_120_4_pansh/TRAINING/INPUT/input_1.h5\n",
      "(120, 120, 5)\n",
      "../2_DATA_GHANA/DATASET/120_x_120_8_ms/TRAINING/INPUT/input_1.h5\n",
      "(120, 120, 9)\n",
      "../2_DATA_GHANA/DATASET/120_x_120_8_pansh_8_ms/TRAINING/INPUT/input_1.h5\n",
      "(120, 120, 17)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "\n",
    "    path_patches='../2_DATA_GHANA/RAW_PATCHES/120_x_120/'\n",
    "    \n",
    "    \n",
    "   \n",
    "    training_ratio=0.8 #so    test_ratio=0.2\n",
    "    validation_ratio=0.2\n",
    "    \n",
    "    \n",
    "    list_input_panchro=[]\n",
    "    list_input_ms=[]\n",
    "    list_input_pansharp=[]\n",
    "    for filename in sorted(os.listdir(path_patches)):\n",
    "        if filename.startswith('panchro'):\n",
    "            print('Reading %s'%filename)\n",
    "            list_input_panchro.append(read_data_h5(path_patches+filename))\n",
    "        if filename.startswith('ms'):\n",
    "            print('Reading %s'%filename)\n",
    "            list_input_ms.append(read_data_h5(path_patches+filename))\n",
    "        if filename.startswith('pansharp'):\n",
    "            print('Reading %s'%filename)\n",
    "            list_input_pansharp.append(read_data_h5(path_patches+filename))\n",
    "        if filename.startswith('groundtruth'):\n",
    "            print('Reading %s'%filename)\n",
    "            list_output=read_data_h5(path_patches+filename)\n",
    "\n",
    "            \n",
    "    ## READ ALL\n",
    "    \n",
    "    list_input_panchro=np.squeeze(np.asarray(list_input_panchro))[newaxis,:,:,:]\n",
    "    list_input_pansharp=np.squeeze(np.asarray(list_input_pansharp))\n",
    "    list_input_ms=np.squeeze(np.asarray(list_input_ms))\n",
    "    list_input=np.concatenate((list_input_panchro,list_input_pansharp,list_input_ms),axis=0)\n",
    "    \n",
    "    list_input=np.transpose(list_input,(1,2,3,0))\n",
    "    list_output=np.squeeze(list_output)\n",
    "    print(list_input.shape)\n",
    "    print(list_output.shape)\n",
    "    \n",
    "    print('Dataset read')\n",
    "    idx_shuffle = np.arange(len(list_input))\n",
    "    np.random.shuffle(idx_shuffle)\n",
    "    print('Dataset shuffled')    \n",
    "    list_input=list_input[idx_shuffle]\n",
    "    list_output=list_output[idx_shuffle]\n",
    "    \n",
    "    print('Save indices of shuffle')\n",
    "    np.savetxt('indices_dataset.txt',idx_shuffle)\n",
    "    #Do the split\n",
    "    training_size=int(round(training_ratio*list_input.shape[0]))\n",
    "    test_size=list_input.shape[0]-training_size\n",
    "    validation_size=int(round(validation_ratio*training_size))\n",
    "    training_size=training_size-validation_size\n",
    "    \n",
    "    \n",
    "    print('Split (TRAINING - VALIDATION:%f) - TEST:%f  done'%(1-validation_ratio,training_ratio))\n",
    "    print('Training size:%d, Validation size:%d, Test size: %d'%(training_size,validation_size,test_size))\n",
    "    \n",
    "    \n",
    "    path_dataset_all=[]\n",
    "    path_dataset='../2_DATA_GHANA/DATASET/120_x_120_8_pansh/'\n",
    "    if not os.path.exists(path_dataset):\n",
    "            os.makedirs(path_dataset)\n",
    "    index=np.arange(9)\n",
    "    path_dataset_all.append([path_dataset,index])\n",
    "    path_dataset='../2_DATA_GHANA/DATASET/120_x_120_4_pansh/'\n",
    "    if not os.path.exists(path_dataset):\n",
    "            os.makedirs(path_dataset)\n",
    "    index=[0,1,2,4,6]\n",
    "    path_dataset_all.append([path_dataset,index])\n",
    "    path_dataset='../2_DATA_GHANA/DATASET/120_x_120_8_ms/'\n",
    "    if not os.path.exists(path_dataset):\n",
    "            os.makedirs(path_dataset)\n",
    "    index=[0,9,10,11,12,13,14,15,16]\n",
    "    path_dataset_all.append([path_dataset,index])\n",
    "            \n",
    "    path_dataset='../2_DATA_GHANA/DATASET/120_x_120_8_pansh_8_ms/'\n",
    "    if not os.path.exists(path_dataset):\n",
    "            os.makedirs(path_dataset)\n",
    "    index=np.arange(17)\n",
    "    path_dataset_all.append([path_dataset,index])\n",
    "           \n",
    "    #     #Save the dataset\n",
    "    for path in path_dataset_all:\n",
    "        path_dataset=path[0]\n",
    "        print(path_dataset)\n",
    "        if not os.path.exists(path_dataset+'TRAINING'):\n",
    "                os.makedirs(path_dataset+'TRAINING')\n",
    "                if not os.path.exists(path_dataset+'TRAINING/INPUT'):\n",
    "                    os.makedirs(path_dataset+'TRAINING/INPUT')\n",
    "                if not os.path.exists(path_dataset+'TRAINING/OUTPUT'):\n",
    "                    os.makedirs(path_dataset+'TRAINING/OUTPUT')\n",
    "        if not os.path.exists(path_dataset+'VALIDATION'):\n",
    "                os.makedirs(path_dataset+'VALIDATION')\n",
    "                if not os.path.exists(path_dataset+'VALIDATION/INPUT'):\n",
    "                    os.makedirs(path_dataset+'VALIDATION/INPUT')\n",
    "                if not os.path.exists(path_dataset+'VALIDATION/OUTPUT'):\n",
    "                    os.makedirs(path_dataset+'VALIDATION/OUTPUT')\n",
    "        if not os.path.exists(path_dataset+'TEST'):\n",
    "                os.makedirs(path_dataset+'TEST')\n",
    "                if not os.path.exists(path_dataset+'TEST/INPUT'):\n",
    "                    os.makedirs(path_dataset+'TEST/INPUT')\n",
    "                if not os.path.exists(path_dataset+'TEST/OUTPUT'):\n",
    "                    os.makedirs(path_dataset+'TEST/OUTPUT')\n",
    "       \n",
    "    \n",
    "\n",
    "    \n",
    "    print('BUILD TRAINING SET')\n",
    "    for i in range(training_size):\n",
    "        print('Patch %d'%i)\n",
    "        for path in path_dataset_all:\n",
    "            write_data_h5(path[0]+'TRAINING/INPUT/input_'+str(i)+'.h5',np.transpose(list_input[i,:,:,path[1]],(1,2,0)))\n",
    "            write_data_h5(path[0]+'TRAINING/OUTPUT/output_'+str(i)+'.h5',list_output[i])\n",
    "        \n",
    "    print('BUILD VALIDATION SET')\n",
    "    for i in range(training_size,training_size+validation_size):\n",
    "        print('Patch %d'%i)\n",
    "        for path in path_dataset_all:\n",
    "            write_data_h5(path[0]+'VALIDATION/INPUT/input_'+str(i)+'.h5',np.transpose(list_input[i,:,:,path[1]],(1,2,0)))\n",
    "            write_data_h5(path[0]+'VALIDATION/OUTPUT/output_'+str(i)+'.h5',list_output[i])\n",
    "        \n",
    "    print('BUILD TEST SET')\n",
    "    for i in range(training_size+validation_size,list_input.shape[0]):\n",
    "        print('Patch %d'%i)\n",
    "        for path in path_dataset_all:\n",
    "            write_data_h5(path[0]+'TEST/INPUT/input_'+str(i)+'.h5',np.transpose(list_input[i,:,:,path[1]],(1,2,0)))\n",
    "            write_data_h5(path[0]+'TEST/OUTPUT/output_'+str(i)+'.h5',list_output[i])\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dhi",
   "language": "python",
   "name": "env_dhi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
